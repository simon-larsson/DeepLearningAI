--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
| 0.  Recurrent Neural Network
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

	Examples of sequence data
	-	Speech recognition / transcribing speech
	-	Music generation
	-	Sentiment classification - Rate move based on comments
	-	DNA sequence analysis - Find a protein in the DNA sequence
	-	Machine translation - Translate between languages
	-	Name entity recognition - Pick out names in texts
	
	
	Notation
	-	Name entity example:
			x: 	Harry Potter and Hermione Granger invented a new spell.
			x<t>:  x<1>  x<2>  x<3>   x<4>    x<5>    x<6>   .  .   x<9>				
			y:	  		1		1	  0		1		  1		 0	    0  0    0
			Tx = 9 - (length of input sequence)		
			Ty = 9 - (length of output sequence)
			x(i)<t> = The t:th element in training example i:s sequence
			
			
	NLP vocabulary
	-	Working with words it is common to have a vocabulary of known words
	-	Commersial vocabularies are around 30k words but some have over millions
	-	Every word in a sequence is then transformed into a one hot encoding of the vocabulary
	-	Example:
			vocab: 			['ass', 'fuck', 'shit']
			x:					"Oh, shit fuck!"
			x_vocab<1>:		<UNK> (unknown word)
			x_vocab<2>:		[0, 0, 1]
			x_vocab<3>:		[0, 1, 0]
			
	
	Problems with standard NNs for sequence data
	-	Inputs and outputs can have different lengths in different examples
	-	Does not share features learned across positions of texts
	
	
	Recurrent Neural Network
	-	Check lecture for better representation of arcitecture
	-	Every timestep as a x<t> that outputs a y_hat<t> which then is feed to the next timestep through a activations functions
	-	a<0> -> [x<1> -> y_hat<1>] -> a<1> -> [x<1> -> y_hat<1>] -> a<1> -> ... -> [x<Tx> -> y_hat<Ty>]
	-	The parameters for each [x<t> -> y_hat<t>] are the same for all timesteps and is denoted Wax
	-	a<t> = g1(Waa*a<t-1> + Wax*x<t> + ba) 	or g1(Wa[a<t-1>, x<t>] + ba)	(where g1 = tanh / sometimes RelU)
	-	y<t> = g2(Wya*a<t> + by) 					or g2(Wy*a<t> + by)				(where g2 = sigmoid / softmax / ...)
	
	
	Flow of RNNs
	-	RNNs only uses information from earlier timesteps to make assumptions in later timesteps and not vice versa
	-	Can be fixed with bi-directional RNNs
	-	Example of this weakness with name recognition:
			"Teddy Roosevelt was great"
			"Teddy bears are great"
			Both times the network will only have "Teddy" to work with and misses the useful information in x<2>
			
			
	Backpropagation through time (RNNs)
	-	Graph:	Check lectures
	- 	Loss: 	L<t>(y_hat<t>, y<t>) = -y<t>*log(y_hat<t>) - (1 - y<t>)*log(1 - y_hat<t>)
					L(y_hat, y) = sum(L<t>(y_hat<t>, y<t>)) 	where t -> Ty
		
		
	Types of RNNs
	-	Many-to-many (even): 	Standard RNNs have Ty = Tx where every x<t> outputs y_hat<t>
		Many-to-many (uneven):	Ty != Tx, x<1> -> x<2> -> ... x<Tx> -> y_hat<1> -> y_hat<2> -> ..., y_hat<Ty> (only x<Tx> output y<1> which then continues to y<Ty>)
										e.g. when translating a sentence the word count in orignal and translated does not have to add up
	-	Many-to-one:				Sometimes Ty = 1, e.g. when a scentence is classified (0/1) then it is only outputted with y_hat in the end
	-	One-to-one:					A standard neural net without time component
	-	One-to-many:				x -> y_hat<1> -> y_hat<2> -> ..., y_hat<Ty>
	
	
	Language models and sequence generation
	-	A language model has the task of deciding the probability of a sentence, P(sentence) 
	-	Example: It can differentiate between two similar sounding words to see which one is most likely to be in a model
	
	
	Building a language model
	-	Take a large corpus of text, and tokenize it to a dictionary
	-	One hot encode with all words to a vector, use <UNK> token and perhaps a <EOS> token for end of sentences
	-	Build a RNN model: 
			y_hat<1> outputs the P(y<1> is in vocab)
			y_hat<2> outputs the P(y<2> | y<1>)
			...
			y_hat<Ty> outputs the P(y<1> | ... | y<Ty>)
			Example: "I like food" -> 	y_hat<1> = P("I")
												y_hat<2> = P("Like" | "I")
												y_hat<3> = P("Food" | "Like" | "I")
												
	
	
	
	
	