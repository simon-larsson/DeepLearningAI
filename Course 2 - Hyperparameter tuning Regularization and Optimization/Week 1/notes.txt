--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
| 0. Setting up your Machine Learning Application
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
	
	Dataset split
	-	The classic split is training/development(cross val)/test set in 60%/20%/20%
	-	Still a good model for smaller training sets < 10 000
	- 	In larger sets the dev and training set can be reduced
	-	Example: 1 000 000 training examples can be split to 99%/1%/1%
	
	Bias/Variance
	-	Bias is the model fits the data poorly
	-	Variance is when the model is overfitted to the data set to the point where it will perform worse on a dev/test set
	
	Step By Step
	1	Train the network 
	2	Check for bias
	3 	If bias is found, increase size of network and go to step 1
	4	Check for high variance on the dev set
	5	If high variance is found, if possible get more data and go to step 1
	6 	Lastly try regularization


--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
| 1. Regularizing your neural network
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
		
	L2 Regularization
	-	The regularization term for a neural network is (lamda/2m)*sum(W.^2)
	-	Back prop formula:	dW = (1/m)*dZ.*A.T + (lambda/m)*W
	
	Why does L2 regularization reduce overfitting?
	-	The regularization term (forbineus norm) puts extra emphasis on the weights in the cost function
	-	Large weights get penalized
	-	Smaller weights reduce the effect of the network which makes it less prone to overfitting
	
	Cost
	-	J = sum(L(Y_hat, Y)) + (lamda/2m)*sum(W.^2)
	
	Dropout regularization
	-	Dropout regularization is where you set a dropout probability on each node
	-	At the beginning of training each node gets evaluated randomly with the probability
	-	The selected nodes get eliminated from the network, making it simpler
	
	Implementing inverted dropout for forward propagation
	-	For a layer l we define keep_prob = dropout procentage	(ex: 0.80 for 80%)
		D[l] = np.random.rand(A[l].shape[0], A[l].shape[0]) < keep_prob	Find out which units should be dropped
		A[l] = np.multiply(A[l], D[l])												Drop units by multipling with activation functions
		A[l] = np.divide(A[l], keep_prob)											Divide with keep_prob so that A still has the same scale overall
	
	Implementing inverted dropout for back propagation
   -	dA[l] = np.multiply(dA[l], D[l])          								Drop units by multipling with activation functions
		dA[l] = np.divide(dA[l], keep_prob) 										Divide with keep_prob so that dA still has the same scale overall
	
	Why does inverted dropout regularization reduce overfitting?
	-	Will reduce the network by dropping out units
	-	Smaller networks are less complex and therefore less prone to overfitting
	-	Since it drops out units randomly it prevents the model from relying on one unit too much
	-	Makes it spread out on all weights more and also decrease the overall scale of the weights
	
	Augmenting data
	-	 Not really regularization but helps with the same issue, lack of data
	-	Overfitting can be reduced with augmenting the dataset
	-	Examples:	Adding train examples of images by using existing examples and performing manipulations (flip left/right, cropping, distort)
	
	Early stopping
	-	Stopping the training iterations early
	-	Makes the model less trained and thus less overfitted
	-	Couples the optimzation and the regularization which makes it hard to optimize in general
	
	
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
| 2. Setting up your optimzation problem
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

	Normalizing inputs
	-	Subtract mean:				mu = (1/m)*sum(x(i)) 		x -> m	x := x - mu
	-	Normalizing variance:	sigma = (1/m)*sum(x(i).^2)	x -> m	x := x/(sigma^2)
	-	Use same mu and sigma for both training and test set
	
	Why normalize?
	-	If the features have different scaling then the cost function gets distorted
	-	Distorted cost functions train slower
	
	Vanishing / Exploding gradients
	-	Very deep neural networks can sometimes get very large or small gradients which makes them slow
	-	This is due to unbalance in the weights growing exponentially through the layers
	
	Weight initialization for deep networks
	-	When handling many inputs the weights need to be smaller to keep scale since the inputs are summed up
	-	To remedy this it is smart to make the weight initialization proportional to the number of inputs for the given layer
	-	Formula when g is tanh:	W[l] = np.random.randn(n[l], n[l-1])*np.sqrt(1/n[l-1])	(Xaviar initialization)
	-	Formula when g is relu:	W[l] = np.random.randn(n[l], n[l-1])*np.sqrt(2/n[l-1])	(He intialization)
	
	Gradient checking
	-	Used to double check that the gradients of a network is correct
	-	Take all W[l] and b[l] and reshape them to one long vector Theta
	-	Take all dW[l] and db[l] and reshape them to one long vector dTheta
	-	Now the cost is instead J(Theta) which can be compared with the real J to see if they are close (within ~10^-7)
	
	Gradient checking implementation
		 parameters_values, _ 	= dictionary_to_vector(W_b_dictionary)	Make parameters into one long array
		 grad 						= gradients_to_vector(gradients)			Make gradients from backprop into one long array
		 num_parameters 			= parameters_values.shape[0]				Find length of parameter array
		 J_plus 						= np.zeros((num_parameters, 1))			Dimension J+
		 J_minus 					= np.zeros((num_parameters, 1))			Dimension J-
		 gradapprox 				= np.zeros((num_parameters, 1))			Gradient approximation
		 
			 for i in range(num_parameters):												
				  
				  thetaplus 			= np.copy(parameters_values)													                                     
				  thetaplus[i][0] 	= thetaplus[i][0] + epsilon   												theta+ = theta + epsilon                                 		
				  J_plus[i], _ 		= forward_propagation(X, Y, vector_to_dictionary(thetaplus))   	Calculate forwardprop with theta-
				  
				  thetaminus 			= np.copy(parameters_values)                                       
				  thetaminus[i][0] 	= thetaminus[i][0] - epsilon                                      theta+ = theta + epsilon    
				  J_minus[i], _ 		= forward_propagation(X, Y, vector_to_dictionary(thetaminus)) 		Calculate forwardprop with theta+

				  
				  gradapprox[i] = (J_plus[i] - J_minus[i])/(2*epsilon)											gradapprox = (J+ - J-)/2epsilon

			 
			 numerator 		= np.linalg.norm(grad - gradapprox)                   
			 denominator 	= np.linalg.norm(grad) + np.linalg.norm(gradapprox)  
			 difference 	= numerator/denominator                               							|| grad - gradapprox ||/(|| grad || - || gradapprox ||)	
	