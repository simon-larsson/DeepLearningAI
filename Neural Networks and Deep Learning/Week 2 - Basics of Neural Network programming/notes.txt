--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
| 0. Logistic Regression as a Neural Network
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
	
	General notation
	-	(x, y)	-	One training example where x is a nx-dimensional vector or (nx x 1)	
	-	m			-	Number of training examples (x, y)
														_____________________________________
	-	X			-	Matrix of features 	=	|	  |		  |	 	  |			|	|	Dimension X = (nx x m)
														|	x^(1)		x^(2) 	,...,		x^(m)	|	Where nx = feature dimension
														|	  |		  |		  |			|	|
														|	  |		  |	 	  |			|	|
														-------------------------------------
	- Y			- Output vector = [ y^(1)		y^(i) 	,...,		y^(i)	]	Dimension Y = (1 x m)
														
	
	Logistic regression notation
	-	w which is a real nx-dimensional vector
	-	b which is a real number
	- 	y which is a real nx-dimensional vector
	-	x which is a real nx-dimensional vector
	-	y (real data) / y_hat (predicted data) 
	-	y_hat = sigmoid(w'x + b)
	-	sigmoid(z) = 1 / (1 + e^-z)
	-	Given {(x^(1), y^(1)), ..., (x^(m), y^(m))}, want y_hat^(i) ≈ y
	-	L = loss function (single training example)
	-	L(y_hat, y) = -(y*log(y_hat) + (1 - y)*log(1 - y_hat))	which gives a convex loss function
	-	J = cost function (loss but for the entire set)
	-	J(w, b) = (1/m)*sum(L(y_hat^(i), y^(i)))	where i => 1 -> m
	
	
	Gradient descent
	- 	Gradient descent is a common optimizer algorithm that is used to reduce J to minimum
	-	Finds the steepest hill down J and goes a step in that direction for every iteration
	-	w := w - α*(∂J(w, b)/∂w) 	where α is the step size
	-	b := b - α*(∂J(w, b)/∂b)
	
	Backwards propagation
	- 	Backprop is a step where the derivative of a computation graph in terms of one of it's components is found by going backwards in the graph
	-	You start end of the graph and work your way backwards until the term which the derivation is dependent on
	- 	Example: 	a = 5, b = 3, c = 2 -> ab = u -> a + u = v -> 3v = j     
		Then:			dj/dv = 3	since we want to dervive dependent on v we have to prop back to 3v where v is found
		Then:			dj/da = dj/dv * dv/da 	since j = 3(a + u) we outer product dj/dv = 3 and an inner product dv/da = 1
	
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
| 1. Python and Vectorization
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
	
	Numpy vectorization
	-	z = w'x + b		=>		z = np.dot(w, x) + b
	
	Semi vectorized logistic regression:
	-	J = 0
		for i = 1 to m:
			z = np.dot(w', *x[i]) + b
			a = sigmoid(x[i])
			J += -[y[i]*log(y_hat[i]) + (1-y[i])*log(1-y_hat[i])]
			dz = a*(1-a)
			dw += x[i]*dz
			db += dz
		J = J/m
		dw = dw/m
		db = db/m		
		
	Vectorized logistic regression
	- 	Z 	= [z^(1), z^(2), ..., z^(m)] 	1 x m in dimension
	-	B 	= [b, b, ..., b]					1 x m in dimension
	- 	Z 	= w'*X + B
	-	Z 	= np.dot(w', x) + b
	-	A 	= [a^(1), a^(2), ..., a^(m)] 	1 x m in dimension
	-	A 	= sigmoid(Z)
	-	dZ	= A - Y
	-	db = (1/m)*np.sum(dZ)
	-	dw = (1/m)*X*dZ'
	
	
	Vectorized impementation
	-	Z 	= np.dot(w.T, X) + b
	-	A 	= sigmoid(Z)				(Where A contains all the predictions)
	-	dZ = A - Y
	-	dw	= (1/m)*X*dZ.T
	-	db = (1/m)*np.sum(dZ)
	-	w	= w - alpha*dw 
	-	b	= b - alpha*db
	
	