--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
| 0. Logistic Regression as a Neural Network
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
	
	General notation
	-	(x, y)	-	One training example where x is a nx-dimensional vector or (nx x 1)	
	-	m			-	Number of training examples (x, y)
														_____________________________________
	-	X			-	Matrix of features 	=	|	  |		  |	 	  |			|	|	Dimension X = (nx x m)
														|	x^(1)		x^(2) 	,...,		x^(m)	|	Where nx = feature dimension
														|	  |		  |		  |			|	|
														|	  |		  |	 	  |			|	|
														-------------------------------------
	- Y			- Output vector = [ y^(1)		y^(i) 	,...,		y^(i)	]	Dimension Y = (1 x m)
														
	
	Logistic regression notation
	-	w which is a real nx-dimensional vector
	-	b which is a real number
	- 	y which is a real nx-dimensional vector
	-	x which is a real nx-dimensional vector
	-	y (real data) / y_hat (predicted data) 
	-	y_hat = sigmoid(w.T*x + b)
	-	sigmoid(z) = 1 / (1 + e^-z)
	-	Given {(x^(1), y^(1)), ..., (x^(m), y^(m))}, want y_hat^(i) ≈ y
	-	L = loss function (single training example)
	-	L(y_hat, y) = -(y*log(y_hat) + (1 - y)*log(1 - y_hat))	which gives a convex loss function
	-	J = cost function (loss but for the entire set)
	-	J(w, b) = (1/m)*sum(L(y_hat^(i), y^(i)))	where i => 1 -> m
	
	
	Gradient descent
	- 	Gradient descent is a common optimizer algorithm that is used to reduce J to minimum
	-	Finds the steepest hill down J and goes a step in that direction for every iteration
	-	w := w - α*(∂J(w, b)/∂w) 	where α is the step size
	-	b := b - α*(∂J(w, b)/∂b)
	
	Backwards propagation
	- 	Backprop is a step where the derivative of a computation graph in terms of one of it's components is found by going backwards in the graph
	-	You start end of the graph and work your way backwards until the term which the derivation is dependent on
	- 	Example: 	a = 5, b = 3, c = 2 -> ab = u -> a + u = v -> 3v = j     
		Then:			dj/dv = 3	since we want to dervive dependent on v we have to prop back to 3v where v is found
		Then:			dj/da = dj/dv * dv/da 	since j = 3(a + u) we outer product dj/dv = 3 and an inner product dv/da = 1
	
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
| 1. Python and Vectorization
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
	
	Numpy vectorization
	-	z = w.T*x + b		=>		z = np.dot(w, x) + b
	
	Semi vectorized logistic regression:
	-	J = 0
		for i = 1 to m:
			z = np.dot(w.T, *x[i]) + b
			a = sigmoid(x[i])
			J += -[y[i]*log(y_hat[i]) + (1-y[i])*log(1-y_hat[i])]
			dz = a*(1-a)
			dw += x[i]*dz
			db += dz
		J = J/m
		dw = dw/m
		db = db/m		
	
	
	Vectorized impementation
	-	Z 	= np.dot(w.T, X) + b		(1 x m in dimension)
		A 	= sigmoid(Z)				(Where A, for activation, contains all the predictions)
		dZ = A - Y		
		dw	= (1/m)*X*dZ.T
		db = (1/m)*np.sum(dZ)
		w	= w - alpha*dw 
		b	= b - alpha*db
		J 	= -(1/m)*np.sum(np.dot(Y, np.log(A.T)) + np.dot(1 - Y, np.log(1 - A.T))) 
		
	Broadcasting
	-	Python can reshape values during certain operations when those operations would not work otherwise
	-	Operations:		+, -, *. /
	-	Rules:			A-shape (m, n), B-shape = (1, n), A operation B reshapes B from (1, n) to (m, n) by copying itself m times
	-	Rules:			A-shape (m, n), B-shape = (m, 1), A operation B reshapes B from (m, 1) to (m, n) by copying itself n times
	
	1
	
	A neuron computes the mean of all features before applying the output to an activation function

	x -> A neuron computes a linear function (z = Wx + b) followed by an activation function
	
	