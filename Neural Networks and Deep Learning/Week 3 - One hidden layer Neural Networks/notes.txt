--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
| 0. Shallow Neural Network
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
	
	NEURAL NETWORKS

	Layers
	-	Input layer		-	The inputs from the dataset X, also called layer 0 and is not counted in the total number of layers
	-	Output layer	-	responsible for generating the predicted value Y_hat / A[L]
	-	Hidden layer	-	intermediate layers with parameters not directly visible in the dataset


	Denoting individual units in a layer
	-	z[1](2)	 	= Calculated weight and biases of the second unit in the first layer
	-	a[2](1) 		= Calculated activation of the first unit in the second layer
	
	
	Example network: Input layer = 3 units / hidden layer = 4 units / output layer = 1 unit
	-	Input layer:	x(1)
							x(2)
							x(3)
		
	-	Hidden layer:	z[1](1) = w[1](1).T*X + b[1](1), 	a[1](1) = g(z[1](1))
							z[1](2) = w[1](2).T*X + b[1](2), 	a[1](2) = g(z[1](2))
							z[1](3) = w[1](3).T*X + b[1](3), 	a[1](3) = g(z[1](3))
							z[1](4) = w[1](4).T*X + b[1](4), 	a[1](4) = g(z[1](4))
							
	- Output layer:	z[2] 	= w[2].T*A[1] + b[2], a[2]/y_hat = g(z[2])
	
	
	Matrix notation:
	
	-	X	 = 	|	x(1)	x(2)		...	x(m)	|		Where each row is one of the nx features
					|	x(1)	x(2)		...	x(m)	|
					|	...	...		...	... 	|		
					|	x(1)	x(2)		...	x(m) 	|
	
	-	Z[i] = 	|	z[1](1)	z[1](2)		...	z[1](m)	|		where i is n[l], the number of units
					|	z[2](1)	z[2](2)		...	z[2](m) 	|
					|	...		...	  	  	...	... 		|		
					|	z[i](1)	z[i](2)		...	z[i](m) 	|
					
	-	A[i] = 	|	a[1](1)	a[1](2)		...	a[1](m)	|		where i is n[l], the number of units
					|	a[2](1)	a[2](2)		...	a[2](m) 	|
					|	...		...	  	  	...	... 		|		
					|	a[i](1)	a[i](2)		...	a[i](m) 	|
	
	
	Forward propagation through layers (vectorized)
	-	Layer 1:		Z[1] = W[1]*X + b[1]				where X = A[0]
						A[1] = g(Z[1])
	
		Layer 2:		Z[2] = W[2]*A[1] + b[2]
						A[2] = g(Z[2])
						...
		Layer l:		Z[L] = W[L]*A[L-1] + b[L]
						A[L] = g(Z[L])						where A[L] = Y_hat
						
			
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

	ACTIVATION FUNCTIONS
			
	Activation functions
	-	Activation functions are denoted g(z)
	-	It's non-linear function where the sigmoid function is just one example
	-	One function that is almost always better than sigmoid is a = tanh(z) or a = (exp(z) - exp(-z))/(exp(z) + exp(-z))
	-	Activation function function for the output layer is special since it can prepare for the desired output
	-	ReLU is also a common activation function and is defined as a = max(0, z)
						
						
	Activation function breakdown
	-	Sigmoid		-	should never be used unless it is in the last layer and the output should be 0-1
	-	Tanh			-	superior version alternative to sigmoid except the example above
	-	ReLU			-	the most popular function for hidden layers, learns faster than the above because of derivatives never being close to 0
	-	LeakyReLU	-	same as ReLU but with a small slope for negative values where regular ReLU has derivative that is 0
	
	
	Purpose of activation functions
	-	Without non-linear functions between layers the output will always be a linear function regardless of the number of layers
	-	All hidden layers would be pointless
	-	A linear activation function can be used in the output layer for regression problems
	
	Derivatives of activation functions
	-	Sigmoid		- g(z) = 1/(1 + exp(-z))									dg/dz = (1 + exp(-1))*(1 - (1 + exp(-1)))
	-	Tanh			- g(z) = (exp(z) - exp(-z))/(exp(z) + exp(-z))		dg/dz = 1 - (1 + exp(-1))*(1 - (1 + exp(-1)))^2
	-	ReLU			- g(z) = max(0, z)											dg/dz = 0 if z < 0, 		1 if z > 0, 	undefined if z = 0
	-	Leaky ReLU	- g(z) = max(0.01*z, z)										dg/dz = 0 if z < 0.01, 	1 if z > 0, 	undefined if z = 0
	
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

	GRADIENT DESCENT / BACKPROP
	
	Parameters
	-	W[l]	-	Dimension = (n[l] x n[l - 1])
	-	b[l] 	-	Dimension = (n[l] x 1)
	
	
	Gradient descent
	-	dW[l]	= dJ/dW[l]
	-	db[l] = dJ/db[l]
	-	W[l]	= W[l] - alpha*dW[l]
	-	b[l] 	= b[l] - alpha*db[l]
	-	J(Y_hat, Y) = -(1/m)*sum(L(Y_hat, Y))		
	-	L(Y_hat, Y) = Y*log(Y_hat.T)) + (1 - Y)*log(1 - Y_hat.T)
	
	
	Backpropagation
	-	Output layer:	dZ[L]			= A[L] - Y						where L is the number of layers
							dW[L] 		= (1/m)*dZ[L].*A[L].T		where .* is elementwise multiplication	np.multiply		
							db[L] 		= (1/m)*sum(dZ[L], axis = 1, keepdims = True)
		
		Last hidden:	dZ[L - 1] 	= W[L].T*dZ[1].*dg[L - 1](Z[L - 1])
							dW[L - 1]  	= (1/m)*dZ[L - 1]*A[L - 1].T
							db[L - 1]  	= (1/m)*sum(dZ[L - 1], axis = 1, keepdims = True)
					
		Intermediate					...
					
		First hidden:	dZ[1] 		= W[2].T*dZ[2].*dg[1](Z[1])
							dW[1]  		= (1/m)*dZ[1]*X.T
							db[1]  		= (1/m)*sum(dZ[1], axis = 1, keepdims = True)
	

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

	INITIALIZATION
	
	Initialize with parameters=0
	-	If W = 0 then the network becomes symmetric and all units will be identical
	-	They will stay identical for every iteration
	-	Network will perform same as a network with one unit per layer
	-	b = 0 is fine
	
	Python initialization
	-	W[1] = np.random.randn(n[l], n[l - 1])*0.01		Scaling weights to a small number is faster with some activation functions (sigmoid and such)
	-	b[1] = np.zeros(n[l], 1)
	
	
	